# Guide de l'Assistant IA

L'**Assistant IA** est un chatbot intelligent int√©gr√© √† toutes les pages de l'application AI CFO Suite Phoenix. Il utilise la technologie RAG (Retrieval-Augmented Generation) pour fournir un support contextuel bas√© sur la documentation compl√®te de la plateforme.

---

## üéØ Fonctionnalit√©s Principales

### 1. Support Technique Contextuel

L'assistant a acc√®s √† toute la documentation de la plateforme et peut r√©pondre √† des questions sur :
- Comment utiliser les diff√©rentes fonctionnalit√©s
- L'architecture technique du syst√®me
- Les bonnes pratiques d'utilisation
- Le d√©pannage des probl√®mes courants

**Exemple de questions :**
- "Comment t√©l√©verser un document ?"
- "Quels sont les formats de fichiers support√©s ?"
- "Comment fonctionne le syst√®me d'agents ?"

### 2. Am√©lioration Automatique des Prompts

Lorsque l'assistant d√©tecte qu'un prompt utilisateur est mal formul√© (trop court, manque de contexte, etc.), il propose automatiquement une version am√©lior√©e.

**Exemple :**
- **Prompt initial :** "agents"
- **Prompt am√©lior√© :** "Quels sont les diff√©rents types d'agents disponibles dans AI CFO Suite et quels sont leurs r√¥les sp√©cifiques ?"

L'utilisateur peut copier le prompt am√©lior√© en un clic.

### 3. Suggestions Contextuelles

En fonction de la page actuelle et du contexte de la conversation, l'assistant propose des questions pertinentes que l'utilisateur pourrait vouloir poser.

**Exemples de suggestions :**
- Sur la page **Documents** : "Quels formats de fichiers sont support√©s ?"
- Sur la page **Dashboard** : "Comment interpr√©ter les KPIs ?"
- Sur la page **Playground** : "Comment formuler une bonne question ?"

### 4. Historique de Conversation

L'assistant garde en m√©moire les 6 derniers messages de la conversation pour maintenir le contexte et fournir des r√©ponses coh√©rentes.

---

## üöÄ Utilisation

### Ouvrir l'Assistant

Cliquez sur le bouton flottant en bas √† droite de l'√©cran (ic√¥ne de bulle de message).

### Poser une Question

1. Tapez votre question dans le champ de saisie
2. Appuyez sur **Entr√©e** ou cliquez sur le bouton d'envoi
3. L'assistant analyse votre question et recherche dans la documentation
4. Une r√©ponse contextuelle s'affiche en quelques secondes

### Utiliser les Suggestions

- Cliquez sur une suggestion pour l'ins√©rer automatiquement dans le champ de saisie
- Modifiez-la si n√©cessaire avant de l'envoyer

### Copier un Prompt Am√©lior√©

Si l'assistant propose un prompt am√©lior√© :
1. Cliquez sur le bouton **Copier** sous le prompt am√©lior√©
2. Collez-le dans le champ de saisie ou ailleurs selon vos besoins

### R√©initialiser la Conversation

Cliquez sur l'ic√¥ne de rafra√Æchissement (‚Üª) dans l'en-t√™te du chat pour effacer l'historique et recommencer une nouvelle conversation.

---

## üîß Configuration Backend

### Charger la Documentation dans le RAG

Pour que l'assistant ait acc√®s √† la documentation, vous devez d'abord charger les fichiers de documentation dans la base de donn√©es vectorielle :

```bash
cd backend
python3 load_documentation.py
```

Ce script :
1. Lit tous les fichiers de documentation (README, guides, etc.)
2. Les d√©coupe en chunks de texte
3. Cr√©e un fichier JSON pr√™t pour l'embedding
4. (Optionnel) Lance le processus d'embedding si configur√©

### Fichiers de Documentation Index√©s

- `README.md` : Documentation principale
- `EXPERT_EVALUATION.md` : √âvaluation et recommandations
- `MIGRATION_PREEMBEDDED.md` : Guide de migration
- `MODIFICATIONS_SUMMARY.md` : R√©sum√© des modifications
- `CHANGELOG.md` : Journal des changements
- `QUICKSTART.md` : Guide de d√©marrage rapide

---

## üß† Fonctionnement Technique

### Architecture

```
User Message
    ‚Üì
Frontend (ChatAssistant.tsx)
    ‚Üì
API Endpoint (/api/v1/assistant/chat)
    ‚Üì
AssistantService
    ‚îú‚îÄ‚Üí RAG Query (recherche dans la documentation)
    ‚îú‚îÄ‚Üí Prompt Enhancement Detection
    ‚îú‚îÄ‚Üí LLM Call (GPT-4o-mini)
    ‚îî‚îÄ‚Üí Suggestions Generation
    ‚Üì
Response (message + enhanced_prompt + suggestions)
```

### Mod√®le LLM

L'assistant utilise **GPT-4o-mini** pour un √©quilibre optimal entre :
- **Performance** : R√©ponses rapides (< 2 secondes)
- **Qualit√©** : R√©ponses pr√©cises et contextuelles
- **Co√ªt** : Mod√®le √©conomique pour un usage intensif

### RAG (Retrieval-Augmented Generation)

1. **Requ√™te utilisateur** ‚Üí Vectorisation
2. **Recherche s√©mantique** dans la collection "documentation"
3. **Top 3 chunks** les plus pertinents r√©cup√©r√©s
4. **Contexte enrichi** fourni au LLM
5. **R√©ponse g√©n√©r√©e** bas√©e sur la documentation r√©elle

---

## üí° Bonnes Pratiques

### Pour les Utilisateurs

1. **Soyez sp√©cifique** : Plus votre question est pr√©cise, meilleure sera la r√©ponse
2. **Utilisez les suggestions** : Elles sont con√ßues pour le contexte actuel
3. **Reformulez si n√©cessaire** : Si la r√©ponse n'est pas satisfaisante, essayez de reformuler
4. **Explorez les prompts am√©lior√©s** : Ils peuvent vous apprendre √† mieux formuler vos questions

### Pour les D√©veloppeurs

1. **Maintenez la documentation √† jour** : L'assistant est aussi bon que la documentation qu'il indexe
2. **Relancez `load_documentation.py`** apr√®s chaque mise √† jour majeure de la doc
3. **Surveillez les logs** : Les erreurs de l'assistant sont logu√©es pour faciliter le d√©bogage
4. **Personnalisez le `system_prompt`** dans `assistant_service.py` selon vos besoins

---

## üîê S√©curit√© et Confidentialit√©

- **Pas de stockage permanent** : Les conversations ne sont pas sauvegard√©es en base de donn√©es
- **Contexte limit√©** : Seuls les 6 derniers messages sont gard√©s en m√©moire
- **Acc√®s public** : L'endpoint `/assistant/chat` est accessible sans authentification (peut √™tre modifi√©)
- **Donn√©es sensibles** : Ne partagez pas d'informations confidentielles dans le chat

---

## üõ†Ô∏è Personnalisation

### Modifier le Comportement de l'Assistant

√âditez le fichier `backend/services/assistant_service.py` et modifiez la variable `self.system_prompt` pour changer :
- Le ton de l'assistant (formel, d√©contract√©, etc.)
- Les domaines d'expertise
- Le format des r√©ponses

### Ajouter de Nouveaux Documents

1. Placez vos fichiers Markdown dans le dossier racine du projet
2. Ajoutez-les √† la liste `doc_files` dans `load_documentation.py`
3. Relancez le script de chargement

### Changer le Mod√®le LLM

Dans `assistant_service.py`, modifiez :
```python
self.model = "gpt-4o-mini"  # Changez pour un autre mod√®le
```

Mod√®les recommand√©s :
- `gpt-4o-mini` : Rapide et √©conomique (par d√©faut)
- `gpt-4o` : Plus puissant mais plus lent
- `gpt-3.5-turbo` : Tr√®s rapide, moins pr√©cis

---

## üìä M√©triques et Monitoring

(√Ä impl√©menter)

Suggestions de m√©triques √† suivre :
- Nombre de conversations par jour
- Temps de r√©ponse moyen
- Taux de satisfaction (avec boutons üëç/üëé)
- Questions les plus fr√©quentes
- Taux d'utilisation des prompts am√©lior√©s

---

## üêõ D√©pannage

### L'assistant ne r√©pond pas

1. V√©rifiez que le backend est en cours d'ex√©cution
2. V√©rifiez la cl√© API OpenAI dans `.env`
3. Consultez les logs du backend : `docker-compose logs backend`

### Les r√©ponses sont hors contexte

1. V√©rifiez que la documentation a √©t√© charg√©e : `python3 load_documentation.py`
2. V√©rifiez que la collection "documentation" existe dans Qdrant

### Erreur "Failed to fetch"

1. V√©rifiez que `VITE_API_BASE_URL` est correctement configur√© dans le frontend
2. V√©rifiez les CORS dans le backend

---

**Auteur :** Manus AI  
**Version :** 3.1.0

